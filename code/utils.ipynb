{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing and post classification analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter as C\n",
    "import re\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv(\"../data/data/all_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all files in models ie logs\n",
    "#[os.path.join(top, file) for top, dirs, files in os.walk(\"./hate-speech-detection/models\") for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all images\n",
    "#os.listdir(\"../data/data/img\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## displaying all rows of df\n",
    "#pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting test data\n",
    "#test_data = pd.read_json(\"../data/data/test.jsonl\", lines=True)\n",
    "#test = test_data.sort_values(by=[\"img\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_accuracys(path):\n",
    "    # opening .accuracys files and getting the file names of the images with the lowest and highest squared error\n",
    "    with open(path) as f:\n",
    "        file = f.read()\n",
    "        file = file.split(\"\\n\")\n",
    "    file = [x.split(\"\\t\")[0].split(\"/\")[5].split(\".\")[0] for x in file if x.startswith(\"..\")]\n",
    "    return file[:20], file[20:] # lowest squared error, highest squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_0_11, bott100_0_11 = open_accuracys(\"./hate-speech-detection/results/models/clf100.0.11.pt.accuracys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_0_b11, bott100_0_b11 = open_accuracys(\"./hate-speech-detection/results/models/clf100.0.b11.pt.accuracys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3b01, bott3b01 = open_accuracys(\"./hate-speech-detection/results/accuracys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_bott = [top100_0_11, bott100_0_11, top100_0_b11, bott100_0_b11, top3b01, bott3b01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_img(lst):\n",
    "    for x in lst:\n",
    "        im = Image.open(\"../data/data/img/\" + str(x) + \".png\")\n",
    "        im.show()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(lst):\n",
    "    return [subsublist for sublist in lst for subsublist in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_counts(lst):\n",
    "    # counting unique words and unique bigrams for all memes ie COUNT([{w1, w2}, {w2,  w3}, {w1, w3}])\n",
    "    # outputs two dicts where either bigram or word is the key and count is the value. both dicts are sorted by value, descending\n",
    "    x = all_data[all_data[\"id\"].isin(lst)]\n",
    "    x = x.text.to_list()\n",
    "    \n",
    "    # getting a list of bigrams for each sentence (including punctuation unfortunately) by splitting on everything that's not a word\n",
    "    # maybe rm punctuation instead?\n",
    "    bigrams = [{(w.lower(), re.split(r\"(\\W+)\", sent)[i+1].lower()) for i, w in enumerate(sent.split(\" \")) if i != (len(sent.split(\" \"))-1)} for sent in x ]\n",
    "    bigrams = flatten_list(bigrams)\n",
    "    \n",
    "    words = [{w.lower() for w in set(re.split(r\"(\\W+)\", sent))} for sent in x]\n",
    "    words = flatten_list(words)\n",
    "    \n",
    "    count_words = C(words)\n",
    "    count_bigrams = C(bigrams)\n",
    "    \n",
    "    return dict(sorted(count_words.items(), key=lambda item: item[1], reverse=True)), dict(sorted(count_bigrams.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_0_11c, bott100_0_11c, top100_0_b11c, bott100_0_b11c, top3b01c, bott3b01c = [get_text_counts(l) for l in top_bott]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 20,\n",
       " \"'\": 9,\n",
       " 'the': 9,\n",
       " ', ': 8,\n",
       " 'and': 7,\n",
       " 'in': 7,\n",
       " 'of': 7,\n",
       " '': 6,\n",
       " 'to': 6,\n",
       " 'you': 6,\n",
       " 'we': 6,\n",
       " 'a': 6,\n",
       " 'islam': 5,\n",
       " 'muslims': 5,\n",
       " 's': 5,\n",
       " 'muslim': 5,\n",
       " 'they': 4,\n",
       " '. ': 4,\n",
       " 'is': 4,\n",
       " 'religion': 4,\n",
       " 'i': 4,\n",
       " 'all': 3,\n",
       " 'people': 3,\n",
       " '? ': 3,\n",
       " 'are': 3,\n",
       " 'no': 3,\n",
       " 'it': 3,\n",
       " 'll': 3,\n",
       " 'but': 3,\n",
       " 'if': 3,\n",
       " 'kill': 3,\n",
       " 'peace': 3,\n",
       " 'were': 3,\n",
       " 'women': 2,\n",
       " 'when': 2,\n",
       " 'killing': 2,\n",
       " 'liberals': 2,\n",
       " 'christians': 2,\n",
       " 'think': 2,\n",
       " 'shit': 2,\n",
       " 'race': 2,\n",
       " 'do': 2,\n",
       " 'hate': 2,\n",
       " 'their': 2,\n",
       " 'too': 2,\n",
       " 'child': 2,\n",
       " 'love': 2,\n",
       " 'only': 2,\n",
       " ', \"': 2,\n",
       " ':': 2,\n",
       " 'our': 2,\n",
       " 'dog': 2,\n",
       " 'can': 2,\n",
       " 'believe': 2,\n",
       " 'them': 2,\n",
       " '\" ': 2,\n",
       " 'what': 2,\n",
       " ' \"': 2,\n",
       " 'have': 2,\n",
       " 'going': 2,\n",
       " 'm': 2,\n",
       " 'hell': 2,\n",
       " 'let': 2,\n",
       " 'with': 2,\n",
       " 'christian': 2,\n",
       " '!': 1,\n",
       " 'beating': 1,\n",
       " 'defend': 1,\n",
       " 'babies': 1,\n",
       " 'pedophilia': 1,\n",
       " 'murdering': 1,\n",
       " 'beheading': 1,\n",
       " 'rape': 1,\n",
       " ',': 1,\n",
       " 'homosexuals': 1,\n",
       " 'rapist': 1,\n",
       " 'molesters': 1,\n",
       " ' , ': 1,\n",
       " 'oh': 1,\n",
       " 'good': 1,\n",
       " 'so': 1,\n",
       " 'huh': 1,\n",
       " 'superior': 1,\n",
       " 'who': 1,\n",
       " 'your': 1,\n",
       " 'enemies': 1,\n",
       " 'practice': 1,\n",
       " 'blacks': 1,\n",
       " 'condition': 1,\n",
       " 'look': 1,\n",
       " 'more': 1,\n",
       " 'other': 1,\n",
       " 'at': 1,\n",
       " '!\" ': 1,\n",
       " 'name': 1,\n",
       " 'learn': 1,\n",
       " 'cooking': 1,\n",
       " 'how': 1,\n",
       " 'today': 1,\n",
       " 'ovens': 1,\n",
       " 'use': 1,\n",
       " \"' \": 1,\n",
       " 'german': 1,\n",
       " 'kid': 1,\n",
       " \": '\": 1,\n",
       " 'teacher': 1,\n",
       " 'cannot': 1,\n",
       " 'will': 1,\n",
       " 'mad': 1,\n",
       " 'land': 1,\n",
       " 'buddhist': 1,\n",
       " 'buddhists': 1,\n",
       " '\". -': 1,\n",
       " 'full': 1,\n",
       " 'wirathu': 1,\n",
       " ': \"': 1,\n",
       " 'on': 1,\n",
       " 'weak': 1,\n",
       " 'be': 1,\n",
       " 'become': 1,\n",
       " 'sleep': 1,\n",
       " 'next': 1,\n",
       " 'ashin': 1,\n",
       " 'kindness': 1,\n",
       " 're': 1,\n",
       " 't': 1,\n",
       " 'don': 1,\n",
       " 'criminal': 1,\n",
       " ' / ': 1,\n",
       " 'undocumented': 1,\n",
       " 'calling': 1,\n",
       " 'stop': 1,\n",
       " 'aliens': 1,\n",
       " 'call': 1,\n",
       " 'illegal': 1,\n",
       " 'trespassers': 1,\n",
       " 'immigrants': 1,\n",
       " 'then': 1,\n",
       " 'starting': 1,\n",
       " 'times': 1,\n",
       " 'jackass': 1,\n",
       " 'always': 1,\n",
       " 'f': 1,\n",
       " 'america': 1,\n",
       " 'n': 1,\n",
       " 'terrorists': 1,\n",
       " 'been': 1,\n",
       " 'part': 1,\n",
       " 'back': 1,\n",
       " 'colonial': 1,\n",
       " 'hijackers': 1,\n",
       " 'rights': 1,\n",
       " 'gays': 1,\n",
       " 'take': 1,\n",
       " 'away': 1,\n",
       " 'make': 1,\n",
       " 'legal': 1,\n",
       " 'slavery': 1,\n",
       " 'not': 1,\n",
       " 'that': 1,\n",
       " 'dad': 1,\n",
       " 'son': 1,\n",
       " 'left': 1,\n",
       " 'killed': 1,\n",
       " 'know': 1,\n",
       " 'man': 1,\n",
       " 'some': 1,\n",
       " 'why': 1,\n",
       " 'yes': 1,\n",
       " 'could': 1,\n",
       " 'jews': 1,\n",
       " 'was': 1,\n",
       " 'right': 1,\n",
       " '... ': 1,\n",
       " ': *': 1,\n",
       " 'me': 1,\n",
       " 'door': 1,\n",
       " 'jew': 1,\n",
       " 'locks': 1,\n",
       " 'gas': 1,\n",
       " 'every': 1,\n",
       " 'chamber': 1,\n",
       " '* ': 1,\n",
       " 'vulture': 1,\n",
       " 'bird': 1,\n",
       " 'mostly': 1,\n",
       " 'like': 1,\n",
       " 'weakened': 1,\n",
       " 'species': 1,\n",
       " 'turkey': 1,\n",
       " 'attack': 1,\n",
       " 'big': 1,\n",
       " 'scavengers': 1,\n",
       " 'would': 1,\n",
       " 'prey': 1,\n",
       " 'gear': 1,\n",
       " 'dumb': 1,\n",
       " 'stuff': 1,\n",
       " 'earplugs': 1,\n",
       " 'aviators': 1,\n",
       " 'obsession': 1,\n",
       " '..': 1,\n",
       " 'heads': 1,\n",
       " 'nascar': 1,\n",
       " 'metal': 1,\n",
       " 'trailer': 1,\n",
       " 'trash': 1,\n",
       " 'circles': 1,\n",
       " 'hangovers': 1,\n",
       " 'parking': 1,\n",
       " 'drunk': 1,\n",
       " 'lot': 1,\n",
       " 'fans': 1,\n",
       " 'vests': 1,\n",
       " 'raising': 1,\n",
       " 'getting': 1,\n",
       " 'patches': 1,\n",
       " 'fast': 1,\n",
       " 'nation': 1,\n",
       " 'predominantly': 1,\n",
       " 'worship': 1,\n",
       " 'hot': 1,\n",
       " '-': 1,\n",
       " 'antichrist': 1,\n",
       " 'non': 1,\n",
       " 'catholics': 1,\n",
       " 'lucifer': 1,\n",
       " 'sun': 1,\n",
       " '! ': 1,\n",
       " 'associate': 1,\n",
       " 'goat': 1,\n",
       " 'terrorism': 1,\n",
       " '\"': 1,\n",
       " '?\" ': 1,\n",
       " 'invaders': 1,\n",
       " 'excessive': 1,\n",
       " 'unwelcome': 1,\n",
       " 'normal': 1,\n",
       " 'preaching': 1,\n",
       " 'fuckers': 1,\n",
       " '20': 1,\n",
       " 'there': 1,\n",
       " 'breeders': 1,\n",
       " '7': 1,\n",
       " 'paedophilia': 1,\n",
       " '100': 1,\n",
       " '10': 1,\n",
       " 'money': 1,\n",
       " '25': 1,\n",
       " '5': 1,\n",
       " 'answer': 1,\n",
       " 'myself': 1,\n",
       " 'asked': 1,\n",
       " '28': 1,\n",
       " 'give': 1,\n",
       " 'missing': 1,\n",
       " '] ': 1,\n",
       " 'noise': 1,\n",
       " 'mentions': 1,\n",
       " 'logic': 1,\n",
       " 'god': 1,\n",
       " 'liberal': 1,\n",
       " '101': 1,\n",
       " 'his': 1,\n",
       " '\" [': 1,\n",
       " 'kills': 1,\n",
       " 'cricket': 1,\n",
       " 'dozen': 1,\n",
       " 'throats': 1,\n",
       " 'public': 1,\n",
       " 'shoving': 1,\n",
       " 'down': 1,\n",
       " 'infidels': 1,\n",
       " 'unclean': 1,\n",
       " 'dogs': 1,\n",
       " 'britain': 1,\n",
       " '3': 1,\n",
       " 'march': 1,\n",
       " 'demand': 1,\n",
       " '6': 1,\n",
       " 'lost': 1,\n",
       " 'against': 1,\n",
       " 'job': 1,\n",
       " \" '\": 1,\n",
       " 'matter': 1,\n",
       " 'my': 1,\n",
       " 'sniffer': 1,\n",
       " 'racists': 1,\n",
       " 'million': 1,\n",
       " 'zones': 1,\n",
       " 'as': 1,\n",
       " 'say': 1,\n",
       " 'lives': 1,\n",
       " 'free': 1,\n",
       " 'because': 1,\n",
       " 'uses': 1,\n",
       " 'someone': 1,\n",
       " 'hour': 1,\n",
       " 'wifes': 1,\n",
       " 'instead': 1,\n",
       " 'per': 1,\n",
       " 'sex': 1,\n",
       " 'slaves': 1,\n",
       " 'kilometres': 1}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word counts for the 20 memes with the lowest squared error\n",
    "top100_0_11c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_words(w_list, img_list):\n",
    "    # goes through list of memes and check whether they contain words from a certain list\n",
    "    # outputs a dict with word as key and a set of file names as value\n",
    "    \n",
    "    x = all_data[all_data[\"id\"].isin(img_list)].reset_index(drop=True)\n",
    "    w_id = {}\n",
    "    for row in x.iterrows():\n",
    "        split = re.split(r\"(\\W+)\", row[1].text)\n",
    "        img = row[1].id\n",
    "        for w in split:\n",
    "            if w in w_list:\n",
    "                if w not in w_id.keys():\n",
    "                    w_id[w] = set()\n",
    "                    w_id[w].add(img)\n",
    "                else:\n",
    "                    w_id[w].add(img)\n",
    "    return w_id\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'islam': {31409, 35602, 38461, 48091, 61378},\n",
       " 'muslims': {12867, 15690, 17306, 43519, 98125},\n",
       " 'religion': {18640, 38461, 48091, 91486},\n",
       " 'muslim': {17306, 19875, 31409, 43519, 91486}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which memes from the 20 with the lowest squared errors contain the words \"muslim\", \"muslims\", \"islam\", \"religion\"\n",
    "find_words([\"muslim\", \"muslims\", \"islam\", \"religion\"], top100_0_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving test images to separate folder\n",
    "#origin = \"../data/data/\"\n",
    "#destination = \"../data/data/test/\"\n",
    "#os.mkdir(\"../data/data/test/\")\n",
    "#for img in test_data.img.tolist():\n",
    "#    os.rename((origin+img), (destination+img.lstrip(\"img/\")))\n",
    "#    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating new data split\n",
    "# reading only dev and train as test doesn't have labels\n",
    "dev_data = pd.read_json(\"../data/data/dev.jsonl\", lines=True)\n",
    "train_data = pd.read_json(\"../data/data/train.jsonl\", lines=True)\n",
    "all_data = pd.concat([dev_data, train_data], ignore_index=True)\n",
    "\n",
    "all_data['id'] = list(map(lambda x: str(x).zfill(5), all_data['id'])) # making sure all img id's have their full file name even if it starts w 0\n",
    "all_data['set'] = all_data.shape[0] * ['train'] # creating new column, setting all to train to start out\n",
    "testsize = int(((15/100))*all_data.shape[0]) # setting test and validation size\n",
    "valsize = testsize\n",
    "test = testsize * ['test']\n",
    "val = valsize * ['val']\n",
    "all_data = all_data.sample(frac=1).reset_index(drop=True) # shuffle the dataframe \n",
    "all_data.loc[:(testsize-1), 'set'] = test # setting the top k (= testsize) items in the train/test/val column to test\n",
    "all_data.loc[testsize:(testsize+valsize-1), 'set'] = val # same as above but with val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_want_to_over_write_this_file = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file if you really want to but preferably not\n",
    "if \"all_data.csv\" not in os.listdir(\"../data/data/\"):\n",
    "    if i_want_to_over_write_this_file:\n",
    "        all_data.to_csv(\"../data/data/all_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better to read the existing file \n",
    "df = pd.read_csv(\"../data/data/all_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the rows for validation set \n",
    "not_val = df[(df['set'] == 'val') & (df['label'] == 0)].reset_index(drop=True) # not offensive\n",
    "off_val = df[(df['set'] == 'val') & (df['label'] == 1)].reset_index(drop=True) # offensive\n",
    "# for train\n",
    "not_train = df[(df['set'] == 'train') & (df['label'] == 0)].reset_index(drop=True)\n",
    "off_train = df[(df['set'] == 'train') & (df['label'] == 1)].reset_index(drop=True)\n",
    "# for test\n",
    "not_test = df[(df['set'] == 'test') & (df['label'] == 0)].reset_index(drop=True)\n",
    "off_test = df[(df['set'] == 'test') & (df['label'] == 1)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_txt(df, filename):\n",
    "    file = \"../data/data/\" + filename\n",
    "    with open(file, 'w') as f:\n",
    "        for img in df.img.to_list():\n",
    "            f.write(img+\"\\n\")\n",
    "    print(\"Done!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "are_you_sure = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if \"goodMemesList.txt.test\" not in os.listdir(\"../data/data/\"):\n",
    "    if are_you_sure:\n",
    "        write_txt(not_test, \"goodMemesList.txt.test\")\n",
    "        write_txt(off_test, \"hateMemesList.txt.test\")\n",
    "        write_txt(not_train, \"goodMemesList.txt.train\")\n",
    "        write_txt(off_train, \"hateMemesList.txt.train\")\n",
    "        write_txt(not_val, \"goodMemesList.txt.val\")\n",
    "        write_txt(off_val, \"hateMemesList.txt.val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_sure = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"balanced_data.csv\" not in os.listdir(\"../data/data/\"):\n",
    "    if im_sure:\n",
    "        label_1 = df.copy()[df[\"label\"] == 1]\n",
    "        label_0 = df.copy()[df[\"label\"] == 0].sample(n=3300)\n",
    "        balanced = pd.concat([label_1, label_0], ignore_index=True).to_csv(\"../data/data/balanced_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better to read the file instead\n",
    "balanced_df = pd.read_csv(\"../data/data/balanced_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the rows for balanced validation set \n",
    "bal_not_val = balanced_df[(balanced_df['set'] == 'val') & (balanced_df['label'] == 0)].reset_index(drop=True)\n",
    "bal_off_val = balanced_df[(balanced_df['set'] == 'val') & (balanced_df['label'] == 1)].reset_index(drop=True)\n",
    "# and for train\n",
    "bal_not_train = balanced_df[(balanced_df['set'] == 'train') & (balanced_df['label'] == 0)].reset_index(drop=True)\n",
    "bal_off_train = balanced_df[(balanced_df['set'] == 'train') & (balanced_df['label'] == 1)].reset_index(drop=True)\n",
    "# and for test\n",
    "bal_not_test = balanced_df[(balanced_df['set'] == 'test') & (balanced_df['label'] == 0)].reset_index(drop=True)\n",
    "bal_off_test = balanced_df[(balanced_df['set'] == 'test') & (balanced_df['label'] == 1)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"bal.goodMemesList.txt.test\" not in os.listdir(\"../data/data/\"):\n",
    "    if positive:\n",
    "        write_txt(bal_not_test, \"bal.goodMemesList.txt.test\")\n",
    "        write_txt(bal_off_test, \"bal.hateMemesList.txt.test\")\n",
    "        write_txt(bal_not_train, \"bal.goodMemesList.txt.train\")\n",
    "        write_txt(bal_off_train, \"bal.hateMemesList.txt.train\")\n",
    "        write_txt(bal_not_val, \"bal.goodMemesList.txt.val\")\n",
    "        write_txt(bal_off_val, \"bal.hateMemesList.txt.val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_caption_to_file(df, basedir):\n",
    "    for img, text in zip(df.loc[:, \"img\"].to_list(), df.loc[:, \"text\"].to_list()):\n",
    "        filename = basedir + img + \".ocr\"\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(text)\n",
    "    print(\"Done!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_new_captions = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "if write_new_captions:    \n",
    "    write_caption_to_file(off_test, \"/home/gushansad@GU.GU.SE/lt2318-ai/aics-project/data/data/\")\n",
    "    write_caption_to_file(not_test, \"/home/gushansad@GU.GU.SE/lt2318-ai/aics-project/data/data/\")\n",
    "    write_caption_to_file(not_train, \"/home/gushansad@GU.GU.SE/lt2318-ai/aics-project/data/data/\")\n",
    "    write_caption_to_file(off_train, \"/home/gushansad@GU.GU.SE/lt2318-ai/aics-project/data/data/\")\n",
    "    write_caption_to_file(not_val, \"/home/gushansad@GU.GU.SE/lt2318-ai/aics-project/data/data/\")\n",
    "    write_caption_to_file(off_val, \"/home/gushansad@GU.GU.SE/lt2318-ai/aics-project/data/data/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
